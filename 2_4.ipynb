{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwrB7ciMJbyHZer6UgpYeZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshtenorio/mat494-mathmethods-datasci/blob/main/2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.4 Homework\n",
        "Maximum likelihood estimation (MLE) is an effective approach for estimating the parameters of a probability distribution by maximizing a likelihood function.\n",
        "The point in the parameter space that accomplishes this is the MLE.\n",
        "## 2.4.1 MLE for Random Samples\n",
        "Definition 2.4.1.\n",
        "## 2.4.2 Linear Regression\n",
        "Recall that Linear Regression wants to find coefficients that minimize\n",
        "\\begin{equation*}\n",
        "\\sum_i^n(y_i-\\hat{y}_i)^2.\n",
        "\\end{equation*}\n",
        "From a probabilistic point of view (MLE), suppose we have $n$ points, each of which drawn in an independent and identically distributed way from the normal distribution.\n",
        "For a given mean or variance, the probability of those $n$ points being drawn defines the likelihood function, which are just the product of $n$ normal pdfs.\n",
        "Understanding that $y$ is a random variable, $y_i=\\hat{y}_i+\\epsilon$ where $\\epsilon$ is proportional to $N(0,Ïƒ^2)$, we can see that $y_i$ is a normal variable where the mean is a linear function of $x$ and a fixed standard deviation, so $y_i$ is proportional to $N(\\hat{y}_i, \\sigma^2)$.\n",
        "As such, for each $y_i$ we can choose $\\mu$ in the normal distributions as $u_i=\\hat{y}_i$ and so the MLE can be derived, which becomes the least square problem discussed in 1.3."
      ],
      "metadata": {
        "id": "fGSUVLqpj1ni"
      }
    }
  ]
}