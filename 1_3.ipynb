{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLwSC+JjOHyOcYqGp/A8lD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshtenorio/mat494-mathmethods-datasci/blob/main/1_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Homework\n",
        "Linear regression is a useful tool in applications because of its simplicity.\n",
        "In short, models depend linearly on unknown parameters, which means that these are easier to fit than models that are non-linearly related.\n",
        "## 1.3.1 QR Decomposition\n",
        "QR Decomposition is a useful way to solve the linear least squares problem.\n",
        "\n",
        "The key idea is that you have a n x m matrix A, which is composed of an orthonormal basis span(a1, ..., am).\n",
        "Letting Q be a n x m matrix Q=(q1,...,qm) we can write A=QR where R is an upper triangular and each column i of the matrix R contains the coefficients of the linear combination of qⱼ's that produce aᵢ's.\n",
        "\n",
        "Note that aⱼ∈span(q1,...,qi), so column i of R has only zeros below the diagonal."
      ],
      "metadata": {
        "id": "uWXo-D1_NLfy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fly_YuhXL76_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ca2253-2969-46d5-c88d-e9555b45c0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.13234307  1.65169834 -2.16537751  1.19976014  0.77264743]\n",
            " [ 1.33590645  0.25109529 -0.11595815  0.37739557 -0.13005448]\n",
            " [ 0.16570369 -1.24316539 -1.1102972  -0.78104341 -0.49347246]\n",
            " [ 1.03502701  0.02182297 -0.67066276 -0.87364531 -0.81397834]\n",
            " [-0.38623854  0.60313376 -0.57042729 -1.12857168 -0.1954427 ]]\n",
            "[[-0.54512917 -0.5832022  -0.51984     0.2965678  -0.06725552]\n",
            " [ 0.64312803 -0.43765283 -0.09244193  0.28475429  0.55246341]\n",
            " [ 0.07977257  0.59783725 -0.76187209  0.03963171  0.2328244 ]\n",
            " [ 0.49827956 -0.2506941  -0.36293272 -0.36137173 -0.65311581]\n",
            " [-0.18594179 -0.21926761 -0.09514001 -0.83594398  0.4577045 ]]\n",
            "[[ 2.07720141 -0.93934684  0.78915196 -0.69908738 -0.91344771]\n",
            " [ 0.         -1.95409583  0.94303274 -0.86533088 -0.44179336]\n",
            " [ 0.          0.          2.2799496   0.36093148  0.30034614]\n",
            " [ 0.          0.          0.          1.69145457  0.63007952]\n",
            " [ 0.          0.          0.          0.          0.20345954]]\n",
            "\n",
            "\n",
            "[[-1.13234307  0.54782914 -0.41023275 -0.20732681  0.0614344 ]\n",
            " [ 0.          0.85521557 -0.08717577 -0.24640668 -0.24407467]\n",
            " [ 0.          0.         -1.73702998  0.01430433  0.06992791]\n",
            " [ 0.         -0.         -0.         -0.61124386 -0.4115149 ]\n",
            " [-0.         -0.         -0.         -0.          0.09312435]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.random.randn(5,5) # random 5x5 matrix\n",
        "Q, R = np.linalg.qr(A) # QR Decomposition via Gram-Schmidt Algorithm\n",
        "print(A)\n",
        "print(Q)\n",
        "print(R) # note that this is upper triangle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.2 Least-Squares Problems\n",
        "Suppose we want to solve Ax=b, and use Ax to approximate b.\n",
        "First, let A∈R^{nxn} be a matrix and b∈Rⁿ be a vector.\n",
        "We can use QR decomposition to solve the problem, but numpy also comes with a function to return a least-squares solution, which computes the vector x that solves the equation ax = b."
      ],
      "metadata": {
        "id": "SKuF4srmL4Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data points we want to fit a line y=mx+b through\n",
        "# we should expect to see m =~ 1 and b =~ -2\n",
        "x = np.array([0, 1, 2, 3])\n",
        "y = np.array([-2, -1.2, 0.1, 1.1])\n",
        "\n",
        "# rewrite line equation as y=Ap, where A = [ [x 1] ] and p = [ [m], [b] ]\n",
        "A = np.vstack([x, np.ones(len(x))]).T\n",
        "print(A)\n",
        "m, b = np.linalg.lstsq(A, y, rcond=None)[0] # np.lingalg.lstsq\n",
        "m, b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1y5NQgxNXub",
        "outputId": "c374e3f3-5c4f-4d00-e54d-0ef84a57d3f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1.]\n",
            " [1. 1.]\n",
            " [2. 1.]\n",
            " [3. 1.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0599999999999994, -2.0899999999999985)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3.3 Linear Regression\n",
        "The common approach to solving linear regression problems involves finding coefficients that minimize ∑ᵢⁿ(yi-̂yi)^2.\n",
        "This can be transformed into min(y-Aβ)^2, which is a least squares problem as described in 1.3.2.\n",
        "\n",
        "## License\n",
        "The MIT License (MIT)\n",
        "\n",
        "Copyright (c) 2022 Joshua Tenorio"
      ],
      "metadata": {
        "id": "FqY_daomR8Qx"
      }
    }
  ]
}