{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8s/RY5L8YnWuT9BSwQGQP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshtenorio/mat494-mathmethods-datasci/blob/main/3_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.7 Homework\n",
        "Neural networks are a collection of connected layers of nodes that loosely models the neurons in a biological brain.\n",
        "## 3.7.1 Mathematical Formulation\n",
        "A network consists of inputs, layers (of nodes), a bias, and a forecast output.\n",
        "Each node has a weight associated with each node in the next layer.\n",
        "## 3.7.2 Activation Functions \n",
        "The activation function of a node abstracts the output of that node given a set of inputs.\n",
        "A biological neural network analogy would be electrical signals if a neuron were to fire.\n",
        "Step functions, ReLU function, sigmoid function, softmax function are examples of activation functions.\n",
        "## 3.7.3 Cost Function\n",
        "The least squares can be a practical choice for a cost function.\n",
        "## 3.7.4 Backpropagation\n",
        "Backpropagation is the mechanism for which neural networks are tuned.\n",
        "Specifically it is the practice of fine-tuning the weights of a neural network based on the error rate obtained in the previous iteration.\n",
        "Proper tuning ensures lower error rates which makes the model more reliable.\n",
        "The goal is to minimize the cost function, $J$, with respect to the parameters, the components of $\\mathbf{W}$ and $\\mathbf{b}$.\n",
        "Gradient descent can be used.\n",
        "\n",
        "A backpropagation algorithm utilising gradient descent can look like:\n",
        "1. Initialize weights and biases at random.\n",
        "2. Pick input data and input it to the left side of the network and calculate the output.\n",
        "3. Update the parameters using stochastic graident descent.\n",
        "4. Repeat 2-3 until the desired accuracy is reached."
      ],
      "metadata": {
        "id": "8PdcsP0qYRfX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYfkdVxCYQ7I"
      },
      "outputs": [],
      "source": []
    }
  ]
}